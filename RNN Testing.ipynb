{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN with NodeAnalysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import random\n",
    "import time\n",
    "\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec, KeyedVectors, Phrases\n",
    "\n",
    "from nodeanalysis.NodeAnalysis import NodeAnalysisCallback\n",
    "from nodeanalysis.nodes.EmptyNode import EmptyNode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data...\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "def elapsed(sec):\n",
    "    if sec<60:\n",
    "        return str(sec) + \" sec\"\n",
    "    elif sec<(60*60):\n",
    "        return str(sec/60) + \" min\"\n",
    "    else:\n",
    "        return str(sec/(60*60)) + \" hr\"\n",
    "\n",
    "\n",
    "# Target log path\n",
    "logs_path = 'tmp'\n",
    "#writer = tf.summary.FileWriter(logs_path)#this line was change to the line below by YKU on 05/18/20\n",
    "#writer = tf.train.SummaryWriter(logs_path)\n",
    "# Text file containing words for training\n",
    "#training_file = '..\\outputFiles\\WordCountBytecodeHex.txt' the line was changed to the following line by YKU on 05/18/20\n",
    "#training_file = 'outputFiles/WordCountBytecodeHex.txt'\n",
    "training_file = 'inputFiles/The_D_of_I.txt'\n",
    "#training_file = 'inputFiles/QuickBrownFox.txt'\n",
    "#training_file = 'inputFiles/TwoSentences.txt'\n",
    "#training_file = 'inputFiles/russian_text.txt'\n",
    "def read_data(fname):\n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    content = [word for i in range(len(content)) for word in content[i].split()]\n",
    "    content = np.array(content)\n",
    "    return content\n",
    "\n",
    "training_data = read_data(training_file)\n",
    "print(\"Loaded training data...\")\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary\n",
    "\n",
    "dictionary, reverse_dictionary = build_dataset(training_data)\n",
    "vocab_size = len(dictionary)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 50000\n",
    "display_step = 1000\n",
    "# public static --> void main...\n",
    "# basic words to be used for prediction, coming soon......\n",
    "n_input = 3\n",
    "\n",
    "# number of units in RNN cell\n",
    "n_hidden = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing that I Added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(data, dictt):\n",
    "    return [dictt[data[x]] for x in range(0,len(data))]\n",
    "\n",
    "def group(data, num):\n",
    "    result = []\n",
    "    for i in range(0,len(data)-num+1):\n",
    "        result.append([data[i+x] for x in range(0,num)])\n",
    "    return result\n",
    "\n",
    "def expand(groups, size):\n",
    "    result = []\n",
    "    while len(result) < size:\n",
    "        for x in range(0,len(groups)):\n",
    "            result.append(groups[x])\n",
    "            if len(result) >= size:\n",
    "                break\n",
    "    return result\n",
    "\n",
    "def seperate(groups, index):\n",
    "    x = []\n",
    "    y = []\n",
    "    for a in range(0,len(groups)):\n",
    "        x.append(groups[a][:index])\n",
    "        y.append(groups[a][index:])\n",
    "    return x, y\n",
    "\n",
    "def batches(data, dictt, input_size, batch_size):\n",
    "    x, y = seperate(expand(group(encode(data,dictt),input_size+1),batch_size),input_size)\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to train word 2 vector model...\n",
      "word 2 vector model done training...\n"
     ]
    }
   ],
   "source": [
    "w2v_vector_dim = 25\n",
    "w2v_epochs = 50\n",
    "def w2v_model(data,num):\n",
    "    global w2v_vector_dim\n",
    "    global w2v_epochs\n",
    "\n",
    "    w2v_data = group(data,num)\n",
    "    model = Word2Vec(w2v_data, size=w2v_vector_dim, window=5, min_count=1, workers=4, iter=w2v_epochs)\n",
    "\n",
    "    return model\n",
    "\n",
    "print(\"starting to train word 2 vector model...\")\n",
    "w2v_input_size = 10\n",
    "w2v = w2v_model(training_data, w2v_input_size)\n",
    "w2v_dict = w2v.wv\n",
    "print(\"word 2 vector model done training...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "rnn (RNN)                    (None, 512)               3201024   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 627)               321651    \n",
      "=================================================================\n",
      "Total params: 3,522,675\n",
      "Trainable params: 3,522,675\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "# Add an Embedding layer expecting input vocab of size 1000, and\n",
    "# output embedding dimension of size 64.\n",
    "# model.add(layers.Embedding(vocab_size, 64, input_length=n_input)) <-- replaced by w2v\n",
    "# model.add(layers.Dense(64, input_shape=(3,10)))\n",
    "# Add a LSTM layer with 128 internal units.\n",
    "# model.add(layers.LSTM(128))\n",
    "new_shape = (n_input, w2v_vector_dim,)\n",
    "# model.add(layers.Dense(64, input_shape=new_shape))\n",
    "\n",
    "\n",
    "rnn_cell = tf.keras.layers.StackedRNNCells([tf.keras.layers.LSTMCell(n_hidden),tf.keras.layers.LSTMCell(n_hidden)])\n",
    "# model.add(layers.RNN(rnn_cell, input_length=n_input))\n",
    "model.add(layers.RNN(rnn_cell, input_shape=new_shape))\n",
    "# Add a Dense layer with 10 units.\n",
    "model.add(layers.Dense(len(dictionary)))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3, 25)\n",
      "(1000, 1)\n",
      "# Fit model on training data\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size_param = 1\n",
    "batch_size = 1000*batch_size_param\n",
    "\n",
    "dictt, reverse_dictt = build_dataset(training_data)\n",
    "x_train, _ = batches(training_data, w2v_dict, n_input, batch_size)\n",
    "(x_val, _) = batches(training_data, w2v_dict, n_input, batch_size*0.25)\n",
    "(x_val2, _) = batches(training_data, dictt, n_input, batch_size*0.25)\n",
    "_, y_train = batches(training_data, dictt, n_input, batch_size)\n",
    "_, y_val = batches(training_data, dictt, n_input, batch_size*0.25)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(),  # Optimizer\n",
    "              # Loss function to minimize\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              # List of metrics to monitor\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "print('# Fit model on training data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_path = \"tb25\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logs_path, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodeanalysis Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nac = NodeAnalysisCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 60s 60ms/step - loss: 5.9456 - sparse_categorical_accuracy: 0.0550\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 61s 61ms/step - loss: 5.5736 - sparse_categorical_accuracy: 0.0630 2s - loss: 5.5832\n",
      "Epoch 3/5\n",
      " 435/1000 [============>.................] - ETA: 32s - loss: 5.4379 - sparse_categorical_accuracy: 0.08"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size_param,\n",
    "                    epochs= 5, # epochs,\n",
    "                    # We pass some validation for\n",
    "                    # monitoring validation loss and metrics\n",
    "                    # at the end of each epoch\n",
    "                    #validation_data=(x_val, y_val),\n",
    "                    callbacks=[tensorboard_callback, nac])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_val)\n",
    "\n",
    "for x in range(0,len(predictions)):\n",
    "    prediction = predictions[x]\n",
    "    max = 0\n",
    "    for i in range(0,len(prediction)):\n",
    "        if prediction[i] > prediction[max]:\n",
    "            max = i\n",
    "    print(f'input: { [reverse_dictt[x_val2[x][i]] for i in range(0,len(x_val[x]))] } pred: {reverse_dictt[max]} --- actual: {[reverse_dictt[y_val[x][i]] for i in range(0,len(y_val[x]))] }')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "# plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('model loss')\n",
    "plt.plot(history.history['loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "print(\"duration: \",elapsed(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NodeAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGroup(EmptyNode):\n",
    "    def __init__(self):\n",
    "        super(LSTMGroup, self).__init__()\n",
    "        self.units = None\n",
    "        self.kernel = None\n",
    "        self.recurrent_kernel = None\n",
    "        self.bias_all = None\n",
    "        \n",
    "        self.kernel_dict = None\n",
    "        self.recurrent_kernel_dict = None\n",
    "        self.bias_dict = None\n",
    "        \n",
    "        # weights type\n",
    "        self.KERNEL = \"KERNEL\"\n",
    "        self.RE_KERNEL = \"RECURRENT KERNEL\"\n",
    "    \n",
    "    def report(self):\n",
    "        super(LSTMGroup, self).report()\n",
    "        \n",
    "    def getSimpleNeuronHistory(self, weights_type, node_index, epochs):\n",
    "        h = [getSimpleNeuron(weights_type, node_index) for e in range(epochs)]\n",
    "        return h\n",
    "        \n",
    "    \n",
    "    def getSimpleNeuron(self, weights_type, node_index):\n",
    "        node = SimpleNeuron()\n",
    "        \n",
    "        node.name = f\"{self.name} -- {weights_type} : {node_index}\"\n",
    "        node.model = self.model\n",
    "        node.epoch = self.epoch\n",
    "        \n",
    "        node.layer = self.layer\n",
    "        node.layer_in = self.layer_in\n",
    "        node.layer_out = self.layer_out\n",
    "        \n",
    "        node.layer_index = self.layer_index\n",
    "        node.node_index = [self.node_index, node_index]\n",
    "        node.nac = self.nac\n",
    "        \n",
    "        node.epochs = len(nac.weights[self.layer.name])\n",
    "        \n",
    "        if weights_type == self.KERNEL:\n",
    "            node.weight = self.kernel\n",
    "\n",
    "        if weights_type == self.RE_KERNEL:\n",
    "            node.weight = self.recurrent_kernel\n",
    "            \n",
    "        for index in node_index:\n",
    "            node.weight = node.weight[index]\n",
    "        \n",
    "        node.bias = self.bias_all[node_index[-1]]\n",
    "        \n",
    "        return node\n",
    "        \n",
    "        \n",
    "    def get(self, nac, layer_index, node_index,  epoch=-1):\n",
    "        super(LSTMGroup, self).get(nac, layer_index, node_index, epoch=epoch)\n",
    "        \n",
    "        W = nac.weights[self.layer.name][epoch][node_index*3]\n",
    "        U = nac.weights[self.layer.name][epoch][node_index*3 + 1]\n",
    "        b = nac.weights[self.layer.name][epoch][node_index*3 + 2]\n",
    "        \n",
    "        self.kernel = W\n",
    "        self.recurrent_kernel = U\n",
    "        self.bias_all = b\n",
    "\n",
    "        units = int(int(self.kernel.shape[1])/4)\n",
    "        self.units = units\n",
    "        \n",
    "        # input - forget - cell state - output\n",
    "        W_i = W[:, :units]\n",
    "        W_f = W[:, units: units * 2]\n",
    "        W_c = W[:, units * 2: units * 3]\n",
    "        W_o = W[:, units * 3:]\n",
    "        \n",
    "        self.kernel_dict = {\n",
    "            \"input\" : W_i,\n",
    "            \"forget\" : W_f,\n",
    "            \"cell_state\" : W_c,\n",
    "            \"output\" : W_o\n",
    "        }\n",
    "\n",
    "        U_i = U[:, :units]\n",
    "        U_f = U[:, units: units * 2]\n",
    "        U_c = U[:, units * 2: units * 3]\n",
    "        U_o = U[:, units * 3:]\n",
    "        \n",
    "        self.recurrent_kernel_dict = {\n",
    "            \"input\" : U_i,\n",
    "            \"forget\" : U_f,\n",
    "            \"cell_state\" : U_c,\n",
    "            \"output\" : U_o \n",
    "        }\n",
    "\n",
    "        b_i = b[:units]\n",
    "        b_f = b[units: units * 2]\n",
    "        b_c = b[units * 2: units * 3]\n",
    "        b_o = b[units * 3:]\n",
    "        \n",
    "        self.bias_dict = {\n",
    "            \"input\" : b_i,\n",
    "            \"forget\" : b_f,\n",
    "            \"cell_state\" : b_c,\n",
    "            \"output\" : b_o \n",
    "        }\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = LSTMGroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "group.get(nac,0,0)\n",
    "group.getSimpleNeuron(group.KERNEL, [0,0]).weight\n",
    "# print(group.get(nac, 0, 0).getNode(group.KERNEL, [0,0]).weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = group.getSimpleNeuronHistory(group.KERNEL, [0,0], group.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 Tensorflow 2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
